{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para probar cosas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias que seran utilizadas\n",
    "\n",
    "* sklearn\n",
    "* tensorflow\n",
    "* keras\n",
    "* smart-open\n",
    "* matplotlib\n",
    "* seaborn\n",
    "* numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos a seguir\n",
    "\n",
    "* practicar tutorial del profe\n",
    "* definir forma de acceder a los datos  (**LISTO**)\n",
    "* definir una primera forma de obtener los descriptores del texto   (**LISTO**) -> usando countVectorizer\n",
    "* definir una segunda forma de obtener los descriptores del texto\n",
    "* definir pre procesamiento de los datos (ordenamiento, limpieza, escalado, etc)  (**LISTO**)\n",
    "* definir un regresor para obtener los descriptores de la imagen en base a descriptores de texto (**LISTO**)\n",
    "* entrenar utilizando pocas neuronas escondidas para no demorar tanto y testear resultados\n",
    "* realizar validaciones sobre la red, verificar sobre ajuste y metricas disponibles, iterar cuanto sea necesario.\n",
    "* entrenar para la red mas grande.\n",
    "* definir el calculador de distancias y la mertrica que se utilizara para realizar las busquedas\n",
    "* realizar pruebas sobre los resultados y crear graficos\n",
    "* concluir y generar un informe bonito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografia\n",
    "\n",
    "* http://queirozf.com/entries/evaluation-metrics-for-ranking-problems-introduction-and-examples\n",
    "* https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-regression-ceee5a9eadff\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "* https://datascienceplus.com/keras-regression-based-neural-networks/\n",
    "* tutorial del profe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vayamos agregando los avances aqui, cosa que cuando uno quiera avanzar algo el otro pueda usar su avance y asi.\n",
    "\n",
    "Todos los archivos python que agregues metelos en /src, las funciones de un archivo se importan asi:\n",
    "\n",
    "    from src.foo import function\n",
    "    \n",
    "en /develop meti el codigo de MLP en tensorflow y del java del profe, creo que no los necesitaremos pero los deje por si acaso. Cualquier cosa que creas que podriamos utilizar pero no estes seguro agregala a develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accesso a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo ./data/train_data/train_images_names.txt\n",
      "leyendo ./data/train_data/train_images_vectors.bin\n",
      "20000 vectores de largo 2048\n",
      "(20000,) (20000, 2048)\n",
      "Imagen \"COCO_train2014_000000000086.jpg\" tiene descriptor visual [0.402616   0.14888027 0.09194826 ... 0.08761896 1.0989506  0.31788927] de dimension 2048\n"
     ]
    }
   ],
   "source": [
    "# probemos el codigo del profe para leer datos\n",
    "# cargando descriptores de imagenes\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "def load_file(file_names, file_vectors, num_vectors, vector_dimensions):\n",
    "    assert os.path.isfile(file_names), \"no existe archivo \" + file_names\n",
    "    assert os.path.isfile(file_vectors), \"no existe archivo \" + file_vectors\n",
    "    print(\"leyendo \" + file_names)\n",
    "    names = [line.strip() for line in open(file_names)]\n",
    "    assert num_vectors == len(names), \"no cuadra largo archivo \" + len(names)\n",
    "    print(\"leyendo \" + file_vectors)\n",
    "    mat = numpy.fromfile(file_vectors, dtype=numpy.float32)\n",
    "    vectors = numpy.reshape(mat, (num_vectors, vector_dimensions))\n",
    "    print(str(num_vectors) + \" vectores de largo \" + str(vector_dimensions))\n",
    "    return (names, vectors)\n",
    "\n",
    "def load_train_vectors():\n",
    "    return load_file(\"./data/train_data/train_images_names.txt\", \"./data/train_data/train_images_vectors.bin\", 20000, 2048)\n",
    "    \n",
    "def load_test_vectors():\n",
    "    return load_file(\"test_A_images_names.txt\", \"test_A_images_vectors.bin\", 1000, 2048)\n",
    "\n",
    "(train_names, train_vectors) = load_train_vectors()\n",
    "# (test_names, test_vectors) = load_test_vectors()\n",
    "print(np.shape(train_names), np.shape(train_vectors))\n",
    "print(\"Imagen \\\"\" + train_names[0] + \"\\\" tiene descriptor visual \" + str(train_vectors[0]) + \" de dimension \" + str(len(train_vectors[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n",
      "Imagen \"COCO_train2014_000000000086.jpg\" tiene caption \"Un hombre en un una vieja bicicleta de moda en el bosque\"\n",
      "Imagen \"COCO_train2014_000000000086.jpg\" tiene caption \"Un hombre montado en una bicicleta de motor a través de un bosque.\"\n",
      "Imagen \"COCO_train2014_000000000086.jpg\" tiene caption \"Un hombre sentado en una motocicleta en el bosque.\"\n",
      "Imagen \"COCO_train2014_000000000086.jpg\" tiene caption \"Una persona que mira hacia abajo en algo mientras está sentado en una bicicleta.\"\n",
      "Imagen \"COCO_train2014_000000000086.jpg\" tiene caption \"Una persona joven está en una vieja bicicleta muy adornado.\"\n",
      "Imagen \"COCO_train2014_000000000077.jpg\" tiene caption \"un grupo de adolescentes saltando una rampa en sus monopatines\"\n"
     ]
    }
   ],
   "source": [
    "# cargar captions de train\n",
    "def load_captions(file_captions):\n",
    "    assert os.path.isfile(file_captions), \"no existe archivo \" + file_captions\n",
    "    return [line.strip().split(\"\\t\") for line in open(file_captions, encoding='utf-8')]\n",
    "\n",
    "# test_captions = load_file(\"test_A_captions.txt\")\n",
    "train_captions = load_captions(\"./data/train_data/train_captions.txt\")\n",
    "print(np.shape(train_captions))\n",
    "for i in range(6):\n",
    "    print(\"Imagen \\\"\" + train_captions[i][0] + \"\\\" tiene caption \\\"\" + train_captions[i][1] + \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tengo 20000 vectores de largo 2048, es decir la matriz de entrenamiento y sera de shape (20000, 2048)\n",
    "* pero tengo 5 captions por cada imaganes y por tanto, 5 captions por cada descriptor de imagen, entonces mis shapes seran:\n",
    "\n",
    "    - (100000, $\\alpha$) para la matriz de entrenamiento de entrada (X)\n",
    "    - (100000, 2048) para la matriz de entrenamiento de resultados (y)\n",
    "\n",
    "* donde $\\alpha$ esta dado por el descriptor de texto que creemos. El profe sugirio que seria de largo 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### obtener descriptores de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* countVectorizer usando ngram_range (1, 1) es igual a un unigrama, que es directamente el metodo de bag of words\n",
    "* tf-idf: aun no se lo que es ...\n",
    "* word embedding: es posible implementar esto dentro de la red, pero requiere usar convoluciones dentro de la regresion y para ello se necesita GPU y tal vez agregue mucha complejidad.\n",
    "\n",
    "ahora usare solo bag of words (countVectorizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtengo solo los captions (asumo que los textos vienen ordenados)\n",
    "train_captions = np.array(train_captions)\n",
    "train_texts = train_captions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: (100000, 153)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<100000x153 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 717616 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False,ngram_range=(1,1),max_df=0.8,min_df=0.01,binary=True)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "X_train = vectorizer.transform(train_texts)\n",
    "# X_test  = vectorizer.transform(texts_test)\n",
    "\n",
    "print(\"Entrenamiento: {}\".format(X_train.shape))\n",
    "# print(\"Test: {}\".format(X_test.shape))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ordenemos un poco todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_names, file_vectors, num_vectors, vector_dimensions):\n",
    "    assert os.path.isfile(file_names), \"no existe archivo \" + file_names\n",
    "    assert os.path.isfile(file_vectors), \"no existe archivo \" + file_vectors\n",
    "    print(\"leyendo \" + file_names)\n",
    "    names = [line.strip() for line in open(file_names)]\n",
    "    assert num_vectors == len(names), \"no cuadra largo archivo \" + len(names)\n",
    "    print(\"leyendo \" + file_vectors)\n",
    "    mat = numpy.fromfile(file_vectors, dtype=numpy.float32)\n",
    "    vectors = numpy.reshape(mat, (num_vectors, vector_dimensions))\n",
    "    print(str(num_vectors) + \" vectores de largo \" + str(vector_dimensions))\n",
    "    return (names, vectors)\n",
    "    \n",
    "def load_captions(file_captions):\n",
    "    assert os.path.isfile(file_captions), \"no existe archivo \" + file_captions\n",
    "    return [line.strip().split(\"\\t\") for line in open(file_captions, encoding='utf-8')]\n",
    "\n",
    "# crearemos una clase que simplifique todo el proceso de adquirir los datos\n",
    "class DataHandler(object):\n",
    "    def __init__(self, root_folder=\"./\"):\n",
    "        self.root = root_folder\n",
    "        self.vectors_dimension = 2048\n",
    "        self._data = {}\n",
    "        \n",
    "    def _load(self, names_file, vectors_file, captions_file, num_vectors):\n",
    "        (names, vectors) = load_file(names_file, vectors_file, num_vectors, self.vectors_dimension)\n",
    "        captions = load_captions(captions_file)\n",
    "        vectors_for_captions = []\n",
    "        for vector in vectors:\n",
    "            for _ in range(5):\n",
    "                vectors_for_captions.append(vector)\n",
    "        return names, np.array(vectors_for_captions), np.array(captions)\n",
    "        \n",
    "\n",
    "    def load_train(self, folder=\"\"):\n",
    "        if len(folder) > 0 and folder[0] != '/':\n",
    "            folder = '/' + folder\n",
    "        names_file = \"{}{}/train_images_names.txt\".format(self.root, folder)\n",
    "        captions_file = \"{}{}/train_captions.txt\".format(self.root, folder)\n",
    "        vectors_file = \"{}{}/train_images_vectors.bin\".format(self.root, folder)\n",
    "        return self._load(names_file, vectors_file, captions_file, 20000)\n",
    "    \n",
    "    def load_test(self, folder=\"\"):\n",
    "        if len(folder) > 0 and folder[0] != '/':\n",
    "            folder = '/' + folder\n",
    "        names_file = \"{}{}/test_A_images_names.txt\".format(self.root, folder)\n",
    "        captions_file = \"{}{}/test_A_captions.txt\".format(self.root, folder)\n",
    "        vectors_file = \"{}{}/test_A_images_vectors.bin\".format(self.root, folder)\n",
    "        return self._load(names_file, vectors_file, captions_file, 1000)\n",
    "    \n",
    "    def get_data_count_vectorizer(self, train_folder=\"/train_data\", test_folder=\"/test_A_data\", **kwargs):\n",
    "        train_names, train_vectors, train_image_captions = self.load_train(folder=train_folder)\n",
    "        test_names, test_vectors, test_image_captions = self.load_test(folder=test_folder)\n",
    "        \n",
    "        print(\"doing vectorization with CountVectorizer\")\n",
    "        train_captions = train_image_captions[:,1]\n",
    "        test_captions = test_image_captions[:,1]\n",
    "        vectorizer = CountVectorizer(lowercase=False,ngram_range=(1,1),max_df=0.8,min_df=0.01,binary=True)\n",
    "        print(\"fitting bag of words ...\", end=\"\")\n",
    "        vectorizer.fit(train_captions)\n",
    "        print(\"done\")\n",
    "        \n",
    "        print(\"getting vectors transforms ...\", end=\"\")\n",
    "        train_text_descriptors = vectorizer.transform(train_captions)\n",
    "        test_text_descriptors = vectorizer.transform(test_captions)\n",
    "        print(\"done\")\n",
    "        \n",
    "        # esto podria ser util\n",
    "#         self._data.update({\"train_names\": train_names, \n",
    "#                            \"train_text_descriptors\": train_text_descriptors,\n",
    "#                            \"train_image_descriptors\": train_vectors,\n",
    "#                            \"test_names\": test_names, \n",
    "#                            \"test_text_descriptors\": test_text_descriptors, \n",
    "#                            \"test_image_descriptors\": test_vectors})\n",
    "\n",
    "        return train_text_descriptors, test_text_descriptors, train_vectors, test_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo ./data/train_data/train_images_names.txt\n",
      "leyendo ./data/train_data/train_images_vectors.bin\n",
      "20000 vectores de largo 2048\n",
      "leyendo ./data/test_A_data/test_A_images_names.txt\n",
      "leyendo ./data/test_A_data/test_A_images_vectors.bin\n",
      "1000 vectores de largo 2048\n",
      "doing vectorization with CountVectorizer\n",
      "fitting bag of words ...done\n",
      "getting vectors transforms ...done\n",
      "shapes\n",
      "X_train: (100000, 153)\n",
      "y_train: (100000, 2048)\n",
      "X_test: (5000, 153)\n",
      "y_test: (5000, 2048)\n"
     ]
    }
   ],
   "source": [
    "# luego bastaria con hacer\n",
    "data = DataHandler(root_folder=\"./data\")\n",
    "X_train, X_test, y_train, y_test = data.get_data_count_vectorizer()\n",
    "print(\"shapes\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresor con keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "class Regressor(object):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, n_layers=2, layer_size=2048):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self._n = n_layers\n",
    "        self._size = layer_size\n",
    "        self.optimizer = \"adam\"\n",
    "        self.loss = \"mean_squared_error\"\n",
    "        self.metrics = [\"mae\", \"accuracy\"]\n",
    "        self.model = None\n",
    "        self._regressor = None\n",
    "        \n",
    "    def build(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(self._size, input_dim=self.input_dim, kernel_initializer='normal', activation='relu'))\n",
    "        i = self._n - 1\n",
    "        while i > 0:\n",
    "            self.model.add(Dense(self._size, activation='relu'))\n",
    "            i -= 1\n",
    "        self.model.add(Dense(self.output_dim, activation='linear'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "        \n",
    "    def fit(self, X_train, y_train, epochs=100, batch_size=32, validation_split=0.1, shuffle=True):\n",
    "        results=self.model.fit(X_train,y_train, epochs=epochs, \n",
    "                               validation_split=validation_split,shuffle=shuffle,\n",
    "                               batch_size=batch_size)\n",
    "        return results\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # aqui probablemente debamos normalizar X\n",
    "        return self.model.predict(X)\n",
    "        # y probablemente debamos des-normalizar X e y_predicho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 500)               77000     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2048)              1026048   \n",
      "=================================================================\n",
      "Total params: 1,103,048\n",
      "Trainable params: 1,103,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "90000/90000 [==============================] - 36s 397us/step - loss: 0.5854 - mean_absolute_error: 0.4675 - acc: 0.1579 - val_loss: 0.5817 - val_mean_absolute_error: 0.4710 - val_acc: 0.1808\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================] - 35s 394us/step - loss: 0.5641 - mean_absolute_error: 0.4590 - acc: 0.1802 - val_loss: 0.5774 - val_mean_absolute_error: 0.4630 - val_acc: 0.1875\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================] - 35s 389us/step - loss: 0.5594 - mean_absolute_error: 0.4571 - acc: 0.1841 - val_loss: 0.5764 - val_mean_absolute_error: 0.4666 - val_acc: 0.1859\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================] - 35s 392us/step - loss: 0.5565 - mean_absolute_error: 0.4560 - acc: 0.1883 - val_loss: 0.5760 - val_mean_absolute_error: 0.4590 - val_acc: 0.1896\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================] - 35s 392us/step - loss: 0.5544 - mean_absolute_error: 0.4553 - acc: 0.1912 - val_loss: 0.5753 - val_mean_absolute_error: 0.4610 - val_acc: 0.1901\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================] - 35s 392us/step - loss: 0.5529 - mean_absolute_error: 0.4547 - acc: 0.1921 - val_loss: 0.5760 - val_mean_absolute_error: 0.4614 - val_acc: 0.1913\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================] - 35s 388us/step - loss: 0.5518 - mean_absolute_error: 0.4544 - acc: 0.1938 - val_loss: 0.5756 - val_mean_absolute_error: 0.4631 - val_acc: 0.1908an_absolute_error: 0.4544 - acc\n",
      "Epoch 8/10\n",
      "90000/90000 [==============================] - 35s 391us/step - loss: 0.5508 - mean_absolute_error: 0.4540 - acc: 0.1947 - val_loss: 0.5763 - val_mean_absolute_error: 0.4646 - val_acc: 0.1894\n",
      "Epoch 9/10\n",
      "90000/90000 [==============================] - 36s 395us/step - loss: 0.5500 - mean_absolute_error: 0.4537 - acc: 0.1950 - val_loss: 0.5765 - val_mean_absolute_error: 0.4644 - val_acc: 0.1890\n",
      "Epoch 10/10\n",
      "90000/90000 [==============================] - 35s 384us/step - loss: 0.5493 - mean_absolute_error: 0.4534 - acc: 0.1956 - val_loss: 0.5770 - val_mean_absolute_error: 0.4676 - val_acc: 0.1881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cbe6afc7b8>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esto se debiese llamar simplemente asi: (ejemplo con un regresor pequeño)\n",
    "reg = Regressor(X_train.shape[1], y_train.shape[1], n_layers=1, layer_size=500)\n",
    "reg.build()\n",
    "reg.fit(X_train, y_train, epochs=10)\n",
    "# y luego predecir algun nuevo dato (se va a usar con los datos de test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "sc= MinMaxScaler()\n",
    "X= sc.fit_transform(X)\n",
    "y= y.reshape(-1,1)\n",
    "y=sc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
